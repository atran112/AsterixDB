{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "import glob\n",
    "from natsort import natsorted\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import socket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'requestID': 'e96a4b20-e8c9-49e5-8adb-469b0262a052',\n",
       " 'clientContextID': 'xyz',\n",
       " 'signature': {'*': '*'},\n",
       " 'results': [{'$1': 1}],\n",
       " 'plans': {},\n",
       " 'status': 'success',\n",
       " 'metrics': {'elapsedTime': '46.06746ms',\n",
       "  'executionTime': '44.360947ms',\n",
       "  'resultCount': 1,\n",
       "  'resultSize': 15,\n",
       "  'processedObjects': 0}}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload = {'statement': 'select 1;', 'pretty': 'true', 'client_context_id': 'xyz'}\n",
    "response = requests.get('http://localhost:19002/query/service', params = payload)\n",
    "response.json()\n",
    "# row['metrics']['executionTime'] = float(row['metrics']['executionTime'].replace('ms', ''))\n",
    "# row['metrics']['executionTime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ask user for benchmark type (tiny or big)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter 1 for tiny benchmark or 2 for large benchmark: 1\n",
      "USE TinyBenchmark;\n",
      "\n"
     ]
    }
   ],
   "source": [
    "benchmarkType = input(\"Enter 1 for tiny benchmark or 2 for large benchmark: \")\n",
    "\n",
    "if benchmarkType == \"1\":\n",
    "    isTiny = True\n",
    "elif benchmarkType == \"2\":\n",
    "    isTiny = False\n",
    "else:\n",
    "    print(\"Input is invalid. Defaulting to tiny benchmark.\")\n",
    "    isTiny = True\n",
    "\n",
    "if isTiny:\n",
    "    dataverse = \"USE TinyBenchmark;\"\n",
    "else:\n",
    "    dataverse = \"USE BigBenchmark;\"\n",
    "\n",
    "dataverse += \"\\n\"\n",
    "print(dataverse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the current working directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/andretran/Documents/Projects/AsterixDB/results/'"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.getcwd()\n",
    "cwd += \"/\"\n",
    "cwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the path of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter address: 3\n",
      "Address is not recognized. Defaulting to 127.0.0.1.\n",
      "127.0.0.1\n",
      "/Users/andretran/Documents/Projects/AsterixDB/datasets/\n"
     ]
    }
   ],
   "source": [
    "addr = input(\"Enter address: \")\n",
    "\n",
    "# print(ip)\n",
    "\n",
    "datasetsPath = ''\n",
    "\n",
    "if addr == '127.0.0.1':\n",
    "    datasetsPath = cwd + \"datasets/\"\n",
    "elif addr == '1':\n",
    "    datasetsPath = \"/local_data/downloads/datasets/\"\n",
    "else:\n",
    "    print(\"Address is not recognized. Defaulting to 127.0.0.1.\")\n",
    "    addr = '127.0.0.1'\n",
    "    datasetsPath = cwd + \"datasets/\"\n",
    "\n",
    "print(addr)\n",
    "print(datasetsPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the queries based on if the benchmark is tiny or big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/andretran/Documents/Projects/AsterixDB/queries/*.sql\n",
      "['/Users/andretran/Documents/Projects/AsterixDB/tinyQueries/1a_Create_All.sql', '/Users/andretran/Documents/Projects/AsterixDB/tinyQueries/2a_Load_All.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/3_Point-Count_Buildings.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/4_Point-Count_RoadNetwork.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/5_Point-Count_AllNodes.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/6_Equals_Polygon-Polygon.sql', '/Users/andretran/Documents/Projects/AsterixDB/tinyQueries/7a_Equals_Point-Point.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/8_Disjoint_Polygon-Polygon.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/9_Intersects_Line-Polygon.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/10_Intersects_Point-Polygon.sql', '/Users/andretran/Documents/Projects/AsterixDB/tinyQueries/11a_Intersects_Point-Line.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/12_Touches_Polygon-Polygon.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/13_Touches_Line-Polygon.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/14_Crosses_Line-Line.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/15_Crosses_Line-Polygon.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/16_Overlaps_Polygon-Polygon.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/17_Within_Polygon-Polygon.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/18_Within_Line-Polygon.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/19_Within_Point-Polygon.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/20_Contains_Polygon-Polygon.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/21_Scan_Polygon.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/22_Scan_RoadNetwork.sql', '/Users/andretran/Documents/Projects/AsterixDB/tinyQueries/23a_Create_States.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/24_Range_Buildings-States.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/25_Range_RoadNetwork-States.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/26_Aggregate_Buildings-States.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/27_Aggregate_RoadNetwork-States.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/28_Aggregate_AllNodes-States.sql', '/Users/andretran/Documents/Projects/AsterixDB/tinyQueries/29a_Distance_Points-Points.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/30_Create_UrbanAreas.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/31_ReverseGeocoding_Point-UrbanAreas.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/32_ReverseGeocoding_Point-Street.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/33_MapSearch&Browsing_Point-BoundingBox.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/34_MapSearch&Browsing_Line-BoundingBox.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/35_MapSearch&Browsing_Polygon-BoundingBox.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/36_Land-Information-Management_Buildings-States.sql', '/Users/andretran/Documents/Projects/AsterixDB/queries/37_Land-Information-Management_Average-SqFt-Buildings-States.sql']\n"
     ]
    }
   ],
   "source": [
    "# path = '/Users/andretran/Documents/Projects/AsterixDB/'\n",
    "sqlExt = \"*.sql\"\n",
    "queriesPath = cwd + \"queries/\" + sqlExt\n",
    "print(queriesPath)\n",
    "tinyQueriesPath = cwd + \"tinyQueries/\" + sqlExt\n",
    "bigQueriesPath = cwd + \"bigQueries/\" + sqlExt\n",
    "\n",
    "queries = glob.glob(queriesPath)\n",
    "tinyQueries = glob.glob(tinyQueriesPath)\n",
    "bigQueries = glob.glob(bigQueriesPath)\n",
    "\n",
    "if isTiny:\n",
    "    benchmarkQueries = queries + tinyQueries\n",
    "else:\n",
    "    benchmarkQueries = queries + bigQueries\n",
    "\n",
    "benchmarkQueries = natsorted(benchmarkQueries, key=os.path.basename)\n",
    "print(benchmarkQueries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a results folder to store benchmark results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "resultsPath = cwd + 'results/'\n",
    "\n",
    "isExists = os.path.exists(resultsPath)\n",
    "print(isExists)\n",
    "\n",
    "if not isExists:\n",
    "    os.makedirs(path)\n",
    "    print(\"results directory created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create json and csv files which will hold results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/andretran/Documents/Projects/AsterixDB/results/\n",
      "JSON file created\n",
      "CSV file created\n"
     ]
    }
   ],
   "source": [
    "# Import Module\n",
    "\n",
    "# Folder Path\n",
    "# path = \"/Users/andretran/Documents/Projects/AsterixDB/queries\"\n",
    "\n",
    "# all_files = glob.glob('/Users/andretran/Documents/Projects/AsterixDB/queries/*.sql')\n",
    "\n",
    "# Change the directory\n",
    "# os.chdir(resultsPath)\n",
    "\n",
    "print(resultsPath)\n",
    "\n",
    "# if os.path.exists(cwd + 'results/'):\n",
    "\n",
    "dateString = datetime.utcnow().strftime('%Y-%m-%d %H_%M_%S.%f')[:-3]\n",
    "jsonFile = dateString + \".json\"\n",
    "# jsonFile = \"tinybenchmark.json\"\n",
    "csvFile = dateString + \".csv\"\n",
    "\n",
    "def create_json_file():\n",
    "    if os.path.exists(jsonFile):\n",
    "        os.remove(jsonFile)\n",
    "        print('JSON file deleted')\n",
    "    with open(jsonFile, mode='w', encoding='utf-8') as f:\n",
    "        json.dump([], f)\n",
    "        print('JSON file created')\n",
    "        \n",
    "def create_csv_file():\n",
    "    if os.path.exists(csvFile):\n",
    "        os.remove(csvFile)\n",
    "        print('CSV file deleted')\n",
    "    with open(csvFile, mode='w', encoding='utf-8') as f:\n",
    "        header = ['requestID', 'clientContextID', 'status', 'elapsedTime', 'resultCount', 'resultSize', 'processedObjects', 'timestamp']\n",
    "        writer = csv.writer(f)\n",
    "        # write the header\n",
    "        writer.writerow(header)\n",
    "        print('CSV file created')\n",
    "        \n",
    "create_json_file()\n",
    "create_csv_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the benchmark and save results to csv and json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_queries(file_path, index):\n",
    "    with open(file_path, 'r') as f:\n",
    "        statement = f.read()\n",
    "        if index != 0:\n",
    "            statement = dataverse + statement\n",
    "        if index == 22 or 1:\n",
    "            statement = statement.replace('$PATH$', addr + '://' + datasetsPath)\n",
    "            \n",
    "        print(statement)\n",
    "        payload = {'statement': statement, 'pretty': 'true', 'client_context_id': os.path.basename(file_path), 'readonly': False}\n",
    "        timestamp = datetime.now()\n",
    "        response = requests.post('http://localhost:19002/query/service', params = payload)\n",
    "        row = response.json()\n",
    "        print(row['status'])\n",
    "        with open(jsonFile, \"r\") as of:\n",
    "            data = json.load(of)\n",
    "        data.append(row)\n",
    "        with open(jsonFile, \"w\") as of:\n",
    "            json.dump(data, of)\n",
    "        with open(csvFile, mode='a', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([row['requestID'], row['clientContextID'], row['status'], row['metrics']['elapsedTime'], row['metrics']['resultCount'], row['metrics']['resultSize'], row['metrics']['processedObjects'], timestamp])\n",
    "\n",
    "\n",
    "# iterate through all file\n",
    "for index, file_path in enumerate(benchmarkQueries):\n",
    "    # Check whether file is in text format or not\n",
    "    print(os.path.basename(file_path))\n",
    "    benchmark_queries(file_path, index)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- uniqueID\n",
    "- clientCopntextID as filename\n",
    "- check for status if failed or success\n",
    "- elapsed time\n",
    "- timestamp\n",
    "\n",
    "- create sample output file (json and csv)\n",
    "\n",
    "- collect memory usage over time (timeseries)\n",
    "    - peak memory (how tight a process is in terms of memory comsumption)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc472b739ba3a1e45855b340d200d10d6b8eaf0a9c51ca945a5acc5b9d216ee5"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
