{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from natsort import natsorted\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'requestID': '7499c5e6-c1f3-473e-8564-7fcfc0a907b9',\n",
       " 'clientContextID': 'xyz',\n",
       " 'signature': {'*': '*'},\n",
       " 'results': [{'$1': 1}],\n",
       " 'plans': {},\n",
       " 'status': 'success',\n",
       " 'metrics': {'elapsedTime': '87.850423ms',\n",
       "  'executionTime': '75.830899ms',\n",
       "  'resultCount': 1,\n",
       "  'resultSize': 15,\n",
       "  'processedObjects': 0}}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payload = {'statement': 'select 1;', 'pretty': 'true', 'client_context_id': 'xyz'}\n",
    "response = requests.get('http://localhost:19002/query/service', params = payload)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file deleted\n",
      "file created\n",
      "CSV file deleted\n",
      "file created\n",
      "1_Create_All.sql\n",
      "2_Load_All.sql\n",
      "3_Point-Count_Buildings.sql\n",
      "4_Point-Count_RoadNetwork.sql\n",
      "5_Point-Count_AllNodes.sql\n",
      "6_Equals_Polygon-Polygon.sql\n",
      "7_Equals_Point-Point.sql\n",
      "8_Disjoint_Polygon-Polygon.sql\n",
      "9_Intersects_Line-Polygon.sql\n",
      "10_Intersects_Point-Polygon.sql\n",
      "11_Intersects_Point-Line.sql\n",
      "12_Touches_Polygon-Polygon.sql\n",
      "13_Touches_Line-Polygon.sql\n",
      "14_Crosses_Line-Line.sql\n",
      "15_Crosses_Line-Polygon.sql\n",
      "16_Overlaps_Polygon-Polygon.sql\n",
      "17_Within_Polygon-Polygon.sql\n",
      "18_Within_Line-Polygon.sql\n",
      "19_Within_Point-Polygon.sql\n",
      "20_Contains_Polygon-Polygon.sql\n",
      "21_Scan_Polygon.sql\n",
      "22_Scan_RoadNetwork.sql\n",
      "23_Create_California.sql\n",
      "24_Range_Buildings-California.sql\n",
      "25_Range_RoadNetwork-California.sql\n",
      "26_Aggregate_Buildings-State.sql\n",
      "27_Aggregate_RoadNetwork-State.sql\n",
      "28_Aggregate_AllNodes-State.sql\n",
      "29_Distance_Points-Points.sql\n",
      "30_Create_Avalon.sql\n",
      "31_ReverseGeocoding_Point-City.sql\n",
      "32_ReverseGeocoding_Point-Street.sql\n",
      "33_MapSearch&Browsing_Point-BoundingBox.sql\n",
      "34_MapSearch&Browsing_Line-BoundingBox.sql\n",
      "35_MapSearch&Browsing_Polygon-BoundingBox.sql\n",
      "36_Land-Information-Management_Buildings-State.sql\n",
      "37_Land-Information-Management_Average-SqFt-Buildings-State.sql\n"
     ]
    }
   ],
   "source": [
    "# Import Module\n",
    "\n",
    "# Folder Path\n",
    "path = \"/Users/andretran/Documents/Projects/AsterixDB/queries\"\n",
    "\n",
    "# Change the directory\n",
    "os.chdir(path)\n",
    "jsonFile = \"../tinybenchmark.json\"\n",
    "csvFile = \"../tinybenchmark.csv\"\n",
    "\n",
    "def create_json_file():\n",
    "    if os.path.exists(jsonFile):\n",
    "        os.remove(jsonFile)\n",
    "        print('JSON file deleted')\n",
    "    with open(jsonFile, mode='w', encoding='utf-8') as f:\n",
    "        json.dump([], f)\n",
    "        print('JSON file created')\n",
    "        \n",
    "def create_csv_file():\n",
    "    if os.path.exists(csvFile):\n",
    "        os.remove(csvFile)\n",
    "        print('CSV file deleted')\n",
    "    with open(csvFile, mode='w', encoding='utf-8') as f:\n",
    "        header = ['requestID', 'clientContextID', 'status', 'elapsedTime', 'timestamp']\n",
    "        writer = csv.writer(f)\n",
    "        # write the header\n",
    "        writer.writerow(header)\n",
    "        print('CSV file created')\n",
    "        \n",
    "create_json_file()\n",
    "create_csv_file()\n",
    "\n",
    "\n",
    "def benchmark_queries(file_path, file):\n",
    "    with open(file_path, 'r') as f:\n",
    "        statement = f.read()\n",
    "        payload = {'statement': statement, 'pretty': 'true', 'client_context_id': file, 'readonly': False}\n",
    "        timestamp = datetime.now()\n",
    "        response = requests.get('http://localhost:19002/query/service', params = payload)\n",
    "        row = response.json()\n",
    "        with open(jsonFile, \"r\") as of:\n",
    "            data = json.load(of)\n",
    "        data.append(row)\n",
    "        with open(outputFile, \"w\") as of:\n",
    "            json.dump(data, of)\n",
    "        with open(csvFile, mode='a', encoding='utf-8') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([row['requestID'], row['clientContextID'], row['status'], row['metrics']['elapsedTime'], timestamp])\n",
    "\n",
    "        \n",
    "    \n",
    "\n",
    "# iterate through all file\n",
    "for file in natsorted(os.listdir()):\n",
    "    # Check whether file is in text format or not\n",
    "    if file.endswith(\".sql\"):\n",
    "        file_path = path + \"/\" + file\n",
    "  \n",
    "        # call read text file function\n",
    "        print(file)\n",
    "        benchmark_queries(file_path, file)\n",
    "\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- uniqueID\n",
    "- clientCopntextID as filename\n",
    "- check for status if failed or success\n",
    "- elapsed time\n",
    "- timestamp\n",
    "\n",
    "- create sample output file (json and csv)\n",
    "\n",
    "- collect memory usage over time (timeseries)\n",
    "    - peak memory (how tight a process is in terms of memory comsumption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'requestID': 'c7efb733-ab2a-4a3a-ae6a-ac0a7cf1afcd',\n",
       " 'clientContextID': 'True',\n",
       " 'signature': {'*': '*'},\n",
       " 'errors': [{'code': 1,\n",
       "   'msg': 'ASX0044: DATAVERSE_DROP statement is not supported in read-only mode (in line 1, at column 1)'}],\n",
       " 'status': 'fatal',\n",
       " 'metrics': {'elapsedTime': '2.911723ms',\n",
       "  'executionTime': '279.426Âµs',\n",
       "  'resultCount': 0,\n",
       "  'resultSize': 0,\n",
       "  'processedObjects': 0,\n",
       "  'errorCount': 1}}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "statement = \"DROP DATAVERSE TinyBenchmark IF EXISTS\"\n",
    "payload = {'statement': statement, 'pretty': True, 'client_context_id': 'test', 'readonly': False}\n",
    "response = requests.get('http://localhost:19002/query/service', params = payload)\n",
    "response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-16 01:46:47.789488\n"
     ]
    }
   ],
   "source": [
    "print(datetime.now())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
